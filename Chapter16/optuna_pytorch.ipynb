{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch==1.12.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (1.12.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from torch==1.12.1) (3.7.4.3)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchvision==0.13.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (0.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from torchvision==0.13.1) (3.7.4.3)\n",
      "Requirement already satisfied: requests in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from torchvision==0.13.1) (2.28.1)\n",
      "Requirement already satisfied: numpy in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from torchvision==0.13.1) (1.19.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from torchvision==0.13.1) (9.3.0)\n",
      "Requirement already satisfied: torch==1.12.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from torchvision==0.13.1) (1.12.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from requests->torchvision==0.13.1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from requests->torchvision==0.13.1) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from requests->torchvision==0.13.1) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from requests->torchvision==0.13.1) (2.1.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: matplotlib==3.5.2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (3.5.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from matplotlib==3.5.2) (9.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from matplotlib==3.5.2) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from matplotlib==3.5.2) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from matplotlib==3.5.2) (1.19.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from matplotlib==3.5.2) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from matplotlib==3.5.2) (4.37.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from matplotlib==3.5.2) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from matplotlib==3.5.2) (21.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib==3.5.2) (1.15.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: optuna==2.10.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (2.10.1)\n",
      "Requirement already satisfied: cliff in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from optuna==2.10.1) (4.0.0)\n",
      "Requirement already satisfied: scipy!=1.4.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from optuna==2.10.1) (1.8.1)\n",
      "Requirement already satisfied: numpy in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from optuna==2.10.1) (1.19.5)\n",
      "Requirement already satisfied: cmaes>=0.8.2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from optuna==2.10.1) (0.8.2)\n",
      "Requirement already satisfied: colorlog in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from optuna==2.10.1) (6.7.0)\n",
      "Requirement already satisfied: tqdm in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from optuna==2.10.1) (4.64.1)\n",
      "Requirement already satisfied: PyYAML in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from optuna==2.10.1) (6.0)\n",
      "Requirement already satisfied: alembic in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from optuna==2.10.1) (1.8.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from optuna==2.10.1) (1.4.41)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from optuna==2.10.1) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from packaging>=20.0->optuna==2.10.1) (3.0.9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from sqlalchemy>=1.1.0->optuna==2.10.1) (1.1.3.post0)\n",
      "Requirement already satisfied: Mako in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from alembic->optuna==2.10.1) (1.2.3)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from cliff->optuna==2.10.1) (2.4.2)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from cliff->optuna==2.10.1) (3.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from cliff->optuna==2.10.1) (5.0.0)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from cliff->optuna==2.10.1) (4.0.1)\n",
      "Requirement already satisfied: autopage>=0.4.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from cliff->optuna==2.10.1) (0.5.1)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from cmd2>=1.0.0->cliff->optuna==2.10.1) (1.8.2)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from cmd2>=1.0.0->cliff->optuna==2.10.1) (0.2.5)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from cmd2>=1.0.0->cliff->optuna==2.10.1) (22.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from importlib-metadata>=4.4->cliff->optuna==2.10.1) (3.9.0)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from stevedore>=2.0.1->cliff->optuna==2.10.1) (5.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from Mako->alembic->optuna==2.10.1) (2.1.1)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.12.1\n",
    "!pip install torchvision==0.13.1\n",
    "!pip install matplotlib==3.5.2\n",
    "!pip install optuna==2.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "\n",
    "torch.use_deterministic_algorithms(True)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, trial):\n",
    "        super(ConvNet, self).__init__()\n",
    "        num_conv_layers = trial.suggest_int(\"num_conv_layers\", 1, 4)\n",
    "        num_fc_layers = trial.suggest_int(\"num_fc_layers\", 1, 2)\n",
    "\n",
    "        self.layers = []\n",
    "        input_depth = 1 # grayscale image\n",
    "        for i in range(num_conv_layers):\n",
    "            output_depth = trial.suggest_int(f\"conv_depth_{i}\", 16, 64)\n",
    "            self.layers.append(nn.Conv2d(input_depth, output_depth, 3, 1))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            input_depth = output_depth\n",
    "        self.layers.append(nn.MaxPool2d(2))\n",
    "        p = trial.suggest_float(f\"conv_dropout_{i}\", 0.1, 0.4)\n",
    "        self.layers.append(nn.Dropout(p))\n",
    "        self.layers.append(nn.Flatten())\n",
    "\n",
    "        input_feat = self._get_flatten_shape()\n",
    "        for i in range(num_fc_layers):\n",
    "            output_feat = trial.suggest_int(f\"fc_output_feat_{i}\", 16, 64)\n",
    "            self.layers.append(nn.Linear(input_feat, output_feat))\n",
    "            self.layers.append(nn.ReLU())\n",
    "            p = trial.suggest_float(f\"fc_dropout_{i}\", 0.1, 0.4)\n",
    "            self.layers.append(nn.Dropout(p))\n",
    "            input_feat = output_feat\n",
    "        self.layers.append(nn.Linear(input_feat, 10))\n",
    "        self.layers.append(nn.LogSoftmax(dim=1))\n",
    "        \n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "    \n",
    "    def _get_flatten_shape(self):\n",
    "        conv_model = nn.Sequential(*self.layers)\n",
    "        op_feat = conv_model(torch.rand(1, 1, 28, 28))\n",
    "        n_size = op_feat.data.view(1, -1).size(1)\n",
    "        return n_size\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean and standard deviation values are calculated as the mean of all pixel values of all images in the training dataset\n",
    "train_ds = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,))]))\n",
    "test_ds = datasets.MNIST('../data', train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,))]))\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_ds, batch_size=500, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training and inference routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_prob = model(X)\n",
    "        loss = F.nll_loss(pred_prob, y) # nll is the negative likelihood loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if b_i % 500 == 0:\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                100. * b_i / len(train_dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # loss summed across the batch\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)  # use argmax to get the most likely prediction\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(test_dataloader.dataset)\n",
    "    \n",
    "    accuracy = 100. * success / len(test_dataloader.dataset)\n",
    "\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, success, len(test_dataloader.dataset), accuracy))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define optimizer and model training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    model = ConvNet(trial)\n",
    "    opt_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"Adadelta\", \"RMSprop\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-1, 5e-1, log=True)\n",
    "    optimizer = getattr(optim, opt_name)(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(1, 3):\n",
    "        train(model, device, train_dataloader, optimizer, epoch)\n",
    "        accuracy = test(model, device, test_dataloader)\n",
    "        trial.report(accuracy, epoch)\n",
    "        \n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-30 19:24:56,015]\u001b[0m A new study created in memory with name: mastering_pytorch\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.358899\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 2.392583\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 2.300660\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 2.347017\n",
      "\n",
      "Test dataset: Overall Loss: 2.3457, Overall Accuracy: 1009/10000 (10%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 2.383042\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 2.342031\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 2.360961\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 2.462203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-30 19:29:08,508]\u001b[0m Trial 0 finished with value: 11.35 and parameters: {'num_conv_layers': 2, 'num_fc_layers': 2, 'conv_depth_0': 53, 'conv_depth_1': 33, 'conv_dropout_1': 0.17143248024909297, 'fc_output_feat_0': 55, 'fc_dropout_0': 0.2541003269326993, 'fc_output_feat_1': 26, 'fc_dropout_1': 0.3321122467173282, 'optimizer': 'Adam', 'lr': 0.34442962131036203}. Best is trial 0 with value: 11.35.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test dataset: Overall Loss: 2.3166, Overall Accuracy: 1135/10000 (11%)\n",
      "\n",
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.301533\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.634395\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.780980\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.141913\n",
      "\n",
      "Test dataset: Overall Loss: 0.1789, Overall Accuracy: 9506/10000 (95%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.268348\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.453725\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.525464\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.266002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-30 19:30:25,372]\u001b[0m Trial 1 finished with value: 96.54 and parameters: {'num_conv_layers': 1, 'num_fc_layers': 2, 'conv_depth_0': 32, 'conv_dropout_0': 0.210147972090233, 'fc_output_feat_0': 38, 'fc_dropout_0': 0.23776899365181517, 'fc_output_feat_1': 25, 'fc_dropout_1': 0.3023416454325315, 'optimizer': 'Adadelta', 'lr': 0.14875299308435333}. Best is trial 1 with value: 96.54.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test dataset: Overall Loss: 0.1189, Overall Accuracy: 9654/10000 (97%)\n",
      "\n",
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.327487\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 2.525398\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 2.251005\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 2.353392\n",
      "\n",
      "Test dataset: Overall Loss: 2.3416, Overall Accuracy: 1032/10000 (10%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 2.282689\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 2.371768\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 2.313285\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 2.253626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-30 19:37:43,731]\u001b[0m Trial 2 finished with value: 9.74 and parameters: {'num_conv_layers': 3, 'num_fc_layers': 2, 'conv_depth_0': 36, 'conv_depth_1': 61, 'conv_depth_2': 62, 'conv_dropout_2': 0.3908412995474969, 'fc_output_feat_0': 23, 'fc_dropout_0': 0.2988917276039279, 'fc_output_feat_1': 63, 'fc_dropout_1': 0.38302968688018324, 'optimizer': 'RMSprop', 'lr': 0.31021398410582246}. Best is trial 1 with value: 96.54.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test dataset: Overall Loss: 2.3680, Overall Accuracy: 974/10000 (10%)\n",
      "\n",
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.287229\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 2.305212\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 2.316749\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 2.329492\n",
      "\n",
      "Test dataset: Overall Loss: 2.3033, Overall Accuracy: 1135/10000 (11%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 2.301762\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 2.329828\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 2.280087\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 2.287314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-30 19:42:59,904]\u001b[0m Trial 3 finished with value: 9.58 and parameters: {'num_conv_layers': 4, 'num_fc_layers': 1, 'conv_depth_0': 34, 'conv_depth_1': 39, 'conv_depth_2': 26, 'conv_depth_3': 49, 'conv_dropout_3': 0.17891855958588482, 'fc_output_feat_0': 31, 'fc_dropout_0': 0.3085510868976604, 'optimizer': 'SGD', 'lr': 0.4142508421154321}. Best is trial 1 with value: 96.54.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test dataset: Overall Loss: 2.3052, Overall Accuracy: 958/10000 (10%)\n",
      "\n",
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.271784\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.246002\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.516135\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.552480\n",
      "\n",
      "Test dataset: Overall Loss: 0.1119, Overall Accuracy: 9656/10000 (97%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.243292\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.224081\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.100350\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.535550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-30 19:44:01,530]\u001b[0m Trial 4 finished with value: 97.36 and parameters: {'num_conv_layers': 1, 'num_fc_layers': 1, 'conv_depth_0': 40, 'conv_dropout_0': 0.20849005332387924, 'fc_output_feat_0': 36, 'fc_dropout_0': 0.1884916748930443, 'optimizer': 'SGD', 'lr': 0.31003454731062713}. Best is trial 4 with value: 97.36.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test dataset: Overall Loss: 0.0866, Overall Accuracy: 9736/10000 (97%)\n",
      "\n",
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.303155\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.058255\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.161122\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.232552\n",
      "\n",
      "Test dataset: Overall Loss: 0.0528, Overall Accuracy: 9819/10000 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.071260\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.001816\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.064077\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.012865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-30 19:46:11,006]\u001b[0m Trial 5 finished with value: 98.61 and parameters: {'num_conv_layers': 2, 'num_fc_layers': 1, 'conv_depth_0': 30, 'conv_depth_1': 40, 'conv_dropout_1': 0.1420256489315686, 'fc_output_feat_0': 60, 'fc_dropout_0': 0.19463202740431523, 'optimizer': 'Adadelta', 'lr': 0.14308833731292162}. Best is trial 5 with value: 98.61.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test dataset: Overall Loss: 0.0424, Overall Accuracy: 9861/10000 (99%)\n",
      "\n",
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.304901\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 2.544958\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 2.471034\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 2.371941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-30 19:48:44,003]\u001b[0m Trial 6 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test dataset: Overall Loss: 2.3623, Overall Accuracy: 1028/10000 (10%)\n",
      "\n",
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.308378\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 2.300629\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 2.305186\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 2.332877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-30 19:50:48,121]\u001b[0m Trial 7 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test dataset: Overall Loss: 2.3051, Overall Accuracy: 1028/10000 (10%)\n",
      "\n",
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.337469\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.090801\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.007168\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.094950\n",
      "\n",
      "Test dataset: Overall Loss: 0.0621, Overall Accuracy: 9803/10000 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.123943\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.041031\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.014546\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.043731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-30 19:56:53,612]\u001b[0m Trial 8 finished with value: 98.48 and parameters: {'num_conv_layers': 3, 'num_fc_layers': 1, 'conv_depth_0': 42, 'conv_depth_1': 61, 'conv_depth_2': 59, 'conv_dropout_2': 0.13829291201008012, 'fc_output_feat_0': 32, 'fc_dropout_0': 0.1867735709629219, 'optimizer': 'Adadelta', 'lr': 0.10970838415947894}. Best is trial 5 with value: 98.61.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test dataset: Overall Loss: 0.0445, Overall Accuracy: 9848/10000 (98%)\n",
      "\n",
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.301165\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 2.372999\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 2.288365\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 2.277805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-10-30 19:58:52,389]\u001b[0m Trial 9 pruned. \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test dataset: Overall Loss: 2.3092, Overall Accuracy: 974/10000 (10%)\n",
      "\n",
      "results: \n",
      "num_trials_conducted:  10\n",
      "num_trials_pruned:  3\n",
      "num_trials_completed:  7\n",
      "results from best trial:\n",
      "accuracy:  98.61\n",
      "hyperparameters: \n",
      "num_conv_layers: 2\n",
      "num_fc_layers: 1\n",
      "conv_depth_0: 30\n",
      "conv_depth_1: 40\n",
      "conv_dropout_1: 0.1420256489315686\n",
      "fc_output_feat_0: 60\n",
      "fc_dropout_0: 0.19463202740431523\n",
      "optimizer: Adadelta\n",
      "lr: 0.14308833731292162\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(study_name=\"mastering_pytorch\", direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100, timeout=2000)\n",
    "\n",
    "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
    "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "print(\"results: \")\n",
    "print(\"num_trials_conducted: \", len(study.trials))\n",
    "print(\"num_trials_pruned: \", len(pruned_trials))\n",
    "print(\"num_trials_completed: \", len(complete_trials))\n",
    "\n",
    "print(\"results from best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"accuracy: \", trial.value)\n",
    "print(\"hyperparameters: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
