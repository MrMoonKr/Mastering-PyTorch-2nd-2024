{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torch==1.12\n",
      "  Using cached torch-1.12.0-cp39-none-macosx_10_9_x86_64.whl (133.6 MB)\n",
      "Requirement already satisfied: typing-extensions in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torch==1.12) (4.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: torch 1.12.1\n",
      "    Uninstalling torch-1.12.1:\n",
      "      Successfully uninstalled torch-1.12.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.12.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchvision==0.13 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (0.13.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchvision==0.13) (9.2.0)\n",
      "Requirement already satisfied: requests in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchvision==0.13) (2.28.1)\n",
      "Collecting torch==1.12.0\n",
      "  Using cached torch-1.12.0-cp39-none-macosx_10_9_x86_64.whl (133.6 MB)\n",
      "Requirement already satisfied: numpy in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchvision==0.13) (1.23.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchvision==0.13) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from requests->torchvision==0.13) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from requests->torchvision==0.13) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from requests->torchvision==0.13) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from requests->torchvision==0.13) (3.3)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: torch 1.12.1\n",
      "    Uninstalling torch-1.12.1:\n",
      "      Successfully uninstalled torch-1.12.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.12.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nltk==3.7 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from nltk==3.7) (4.64.0)\n",
      "Requirement already satisfied: joblib in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from nltk==3.7) (1.1.0)\n",
      "Requirement already satisfied: click in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from nltk==3.7) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from nltk==3.7) (2022.7.25)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchtext==0.13.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (0.13.1)\n",
      "Requirement already satisfied: tqdm in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchtext==0.13.1) (4.64.0)\n",
      "Collecting torch==1.12.1\n",
      "  Using cached torch-1.12.1-cp39-none-macosx_10_9_x86_64.whl (133.8 MB)\n",
      "Requirement already satisfied: numpy in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchtext==0.13.1) (1.23.1)\n",
      "Requirement already satisfied: requests in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchtext==0.13.1) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torch==1.12.1->torchtext==0.13.1) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from requests->torchtext==0.13.1) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from requests->torchtext==0.13.1) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from requests->torchtext==0.13.1) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from requests->torchtext==0.13.1) (2.1.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0mInstalling collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: torch 1.12.0\n",
      "    Uninstalling torch-1.12.0:\n",
      "      Successfully uninstalled torch-1.12.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.0 requires torch==1.12.0, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.12.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: networkx==2.8.5 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (2.8.5)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchviz==0.0.2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (0.0.2)\n",
      "Requirement already satisfied: graphviz in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchviz==0.0.2) (0.20.1)\n",
      "Requirement already satisfied: torch in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torchviz==0.0.2) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from torch->torchviz==0.0.2) (4.3.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: matplotlib==3.5.2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (3.5.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib==3.5.2) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib==3.5.2) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib==3.5.2) (4.34.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib==3.5.2) (3.0.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib==3.5.2) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib==3.5.2) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib==3.5.2) (1.23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from matplotlib==3.5.2) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib==3.5.2) (1.16.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-image==0.19.3 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (0.19.3)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from scikit-image==0.19.3) (2.21.1)\n",
      "Requirement already satisfied: networkx>=2.2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from scikit-image==0.19.3) (2.8.5)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from scikit-image==0.19.3) (1.23.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from scikit-image==0.19.3) (2022.8.3)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from scikit-image==0.19.3) (9.2.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from scikit-image==0.19.3) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from scikit-image==0.19.3) (1.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from scikit-image==0.19.3) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages (from packaging>=20.0->scikit-image==0.19.3) (3.0.9)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m1.12.1\n",
      "0.13.0\n",
      "3.7\n",
      "0.13.1\n",
      "2.8.5\n",
      "3.5.2\n",
      "0.19.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.12\n",
    "!pip install torchvision==0.13\n",
    "!pip install nltk==3.7\n",
    "!pip install torchtext==0.13.1\n",
    "!pip install networkx==2.8.5\n",
    "!pip install torchviz==0.0.2\n",
    "!pip install matplotlib==3.5.2\n",
    "!pip install scikit-image==0.19.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eps=10\n",
    "bsize=32\n",
    "lrate=0.001\n",
    "lat_dimension=64\n",
    "image_sz=64\n",
    "chnls=1\n",
    "logging_intv=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GANGenerator, self).__init__()\n",
    "        self.inp_sz = image_sz // 4\n",
    "        self.lin = nn.Linear(lat_dimension, 128 * self.inp_sz ** 2)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.up1 = nn.Upsample(scale_factor=2)\n",
    "        self.cn1 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128, 0.8)\n",
    "        self.rl1 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.up2 = nn.Upsample(scale_factor=2)\n",
    "        self.cn2 = nn.Conv2d(128, 64, 3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64, 0.8)\n",
    "        self.rl2 = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.cn3 = nn.Conv2d(64, chnls, 3, stride=1, padding=1)\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin(x)\n",
    "        x = x.view(x.shape[0], 128, self.inp_sz, self.inp_sz)\n",
    "        x = self.bn1(x)\n",
    "        x = self.up1(x)\n",
    "        x = self.cn1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.rl1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.cn2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.rl2(x)\n",
    "        x = self.cn3(x)\n",
    "        out = self.act(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GANDiscriminator, self).__init__()\n",
    "\n",
    "        def disc_module(ip_chnls, op_chnls, bnorm=True):\n",
    "            mod = [nn.Conv2d(ip_chnls, op_chnls, 3, 2, 1), \n",
    "                   nn.LeakyReLU(0.2, inplace=True), \n",
    "                   nn.Dropout2d(0.25)]\n",
    "            if bnorm:\n",
    "                mod += [nn.BatchNorm2d(op_chnls, 0.8)]\n",
    "            return mod\n",
    "\n",
    "        self.disc_model = nn.Sequential(\n",
    "            *disc_module(chnls, 16, bnorm=False),\n",
    "            *disc_module(16, 32),\n",
    "            *disc_module(32, 64),\n",
    "            *disc_module(64, 128),\n",
    "        )\n",
    "\n",
    "        # width and height of the down-sized image\n",
    "        ds_size = image_sz // 2 ** 4\n",
    "        self.adverse_lyr = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.disc_model(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        out = self.adverse_lyr(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the discriminator and generator models\n",
    "gen = GANGenerator()\n",
    "disc = GANDiscriminator()\n",
    "\n",
    "# define the loss metric\n",
    "adv_loss_func = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab4dc85ce054ebf86b28e00a8cbdc69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b65b959412475cbff66fc5b9dfdbb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24735121bbb84f70953eaecbfdf8c466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62841091c124c94b856f566f5174db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the dataset and corresponding dataloader\n",
    "dloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"./data/mnist/\",\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize((image_sz, image_sz)), \n",
    "             transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=bsize,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# define the optimization schedule for both G and D\n",
    "opt_gen = torch.optim.Adam(gen.parameters(), lr=lrate)\n",
    "opt_disc = torch.optim.Adam(disc.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 0 | batch number 0 | generator loss = 0.7016811370849609 | discriminator loss = 0.691551923751831\n",
      "epoch number 0 | batch number 200 | generator loss = 0.6610271334648132 | discriminator loss = 0.649569034576416\n",
      "epoch number 0 | batch number 400 | generator loss = 1.4007172584533691 | discriminator loss = 0.3584232032299042\n",
      "epoch number 0 | batch number 600 | generator loss = 1.7083189487457275 | discriminator loss = 0.5244866609573364\n",
      "epoch number 0 | batch number 800 | generator loss = 0.8985376954078674 | discriminator loss = 0.6905930042266846\n",
      "epoch number 0 | batch number 1000 | generator loss = 1.15508234500885 | discriminator loss = 0.41335731744766235\n",
      "epoch number 0 | batch number 1200 | generator loss = 2.0180273056030273 | discriminator loss = 0.19367900490760803\n",
      "epoch number 0 | batch number 1400 | generator loss = 1.8608049154281616 | discriminator loss = 0.45206427574157715\n",
      "epoch number 0 | batch number 1600 | generator loss = 1.223183512687683 | discriminator loss = 0.4405233860015869\n",
      "epoch number 0 | batch number 1800 | generator loss = 1.2511184215545654 | discriminator loss = 0.8576059341430664\n",
      "epoch number 1 | batch number 125 | generator loss = 3.4908924102783203 | discriminator loss = 0.10109012573957443\n",
      "epoch number 1 | batch number 325 | generator loss = 1.2453447580337524 | discriminator loss = 0.25381988286972046\n",
      "epoch number 1 | batch number 525 | generator loss = 0.812859058380127 | discriminator loss = 1.1522053480148315\n",
      "epoch number 1 | batch number 725 | generator loss = 1.5032036304473877 | discriminator loss = 0.05369623377919197\n",
      "epoch number 1 | batch number 925 | generator loss = 3.357678174972534 | discriminator loss = 0.2077488750219345\n",
      "epoch number 1 | batch number 1125 | generator loss = 1.5266855955123901 | discriminator loss = 0.20772790908813477\n",
      "epoch number 1 | batch number 1325 | generator loss = 2.838491678237915 | discriminator loss = 0.31947070360183716\n",
      "epoch number 1 | batch number 1525 | generator loss = 4.931915283203125 | discriminator loss = 0.3263084292411804\n",
      "epoch number 1 | batch number 1725 | generator loss = 4.506143569946289 | discriminator loss = 0.1381065845489502\n",
      "epoch number 2 | batch number 50 | generator loss = 1.9775681495666504 | discriminator loss = 0.0863727331161499\n",
      "epoch number 2 | batch number 250 | generator loss = 1.6534879207611084 | discriminator loss = 0.07384362071752548\n",
      "epoch number 2 | batch number 450 | generator loss = 0.9047597050666809 | discriminator loss = 0.14194275438785553\n",
      "epoch number 2 | batch number 650 | generator loss = 2.938152313232422 | discriminator loss = 0.6574025750160217\n",
      "epoch number 2 | batch number 850 | generator loss = 3.537809133529663 | discriminator loss = 0.06796179711818695\n",
      "epoch number 2 | batch number 1050 | generator loss = 5.795485496520996 | discriminator loss = 0.3949619233608246\n",
      "epoch number 2 | batch number 1250 | generator loss = 1.1060248613357544 | discriminator loss = 0.147697314620018\n",
      "epoch number 2 | batch number 1450 | generator loss = 3.8324902057647705 | discriminator loss = 0.09609436243772507\n",
      "epoch number 2 | batch number 1650 | generator loss = 4.295746803283691 | discriminator loss = 0.26552173495292664\n",
      "epoch number 2 | batch number 1850 | generator loss = 2.5446512699127197 | discriminator loss = 0.5005599856376648\n",
      "epoch number 3 | batch number 175 | generator loss = 0.38430485129356384 | discriminator loss = 0.41459140181541443\n",
      "epoch number 3 | batch number 375 | generator loss = 2.141831398010254 | discriminator loss = 0.11136186867952347\n",
      "epoch number 3 | batch number 575 | generator loss = 3.192877769470215 | discriminator loss = 0.33201897144317627\n",
      "epoch number 3 | batch number 775 | generator loss = 2.116788625717163 | discriminator loss = 0.08046729862689972\n",
      "epoch number 3 | batch number 975 | generator loss = 2.072810649871826 | discriminator loss = 0.256913959980011\n",
      "epoch number 3 | batch number 1175 | generator loss = 2.701873779296875 | discriminator loss = 0.09150073677301407\n",
      "epoch number 3 | batch number 1375 | generator loss = 4.171277046203613 | discriminator loss = 0.10688076913356781\n",
      "epoch number 3 | batch number 1575 | generator loss = 1.5341508388519287 | discriminator loss = 0.07717843353748322\n",
      "epoch number 3 | batch number 1775 | generator loss = 5.215853214263916 | discriminator loss = 0.08778893947601318\n",
      "epoch number 4 | batch number 100 | generator loss = 2.4070658683776855 | discriminator loss = 0.1487121284008026\n",
      "epoch number 4 | batch number 300 | generator loss = 2.1141436100006104 | discriminator loss = 0.19630186259746552\n",
      "epoch number 4 | batch number 500 | generator loss = 5.545537948608398 | discriminator loss = 0.10428056120872498\n",
      "epoch number 4 | batch number 700 | generator loss = 5.188838005065918 | discriminator loss = 0.18800003826618195\n",
      "epoch number 4 | batch number 900 | generator loss = 3.338731527328491 | discriminator loss = 0.06498485803604126\n",
      "epoch number 4 | batch number 1100 | generator loss = 2.311957836151123 | discriminator loss = 0.15747112035751343\n",
      "epoch number 4 | batch number 1300 | generator loss = 3.9319167137145996 | discriminator loss = 0.15918156504631042\n",
      "epoch number 4 | batch number 1500 | generator loss = 4.356860160827637 | discriminator loss = 0.02820819616317749\n",
      "epoch number 4 | batch number 1700 | generator loss = 4.762551307678223 | discriminator loss = 0.2548830509185791\n",
      "epoch number 5 | batch number 25 | generator loss = 4.982100486755371 | discriminator loss = 0.03669058531522751\n",
      "epoch number 5 | batch number 225 | generator loss = 3.7877330780029297 | discriminator loss = 0.07158492505550385\n",
      "epoch number 5 | batch number 425 | generator loss = 3.2504031658172607 | discriminator loss = 0.14023567736148834\n",
      "epoch number 5 | batch number 625 | generator loss = 6.316702365875244 | discriminator loss = 0.06633426249027252\n",
      "epoch number 5 | batch number 825 | generator loss = 1.4960943460464478 | discriminator loss = 0.20201145112514496\n",
      "epoch number 5 | batch number 1025 | generator loss = 4.9419264793396 | discriminator loss = 0.009715333580970764\n",
      "epoch number 5 | batch number 1225 | generator loss = 3.9267876148223877 | discriminator loss = 0.23573605716228485\n",
      "epoch number 5 | batch number 1425 | generator loss = 6.590725421905518 | discriminator loss = 0.061152175068855286\n",
      "epoch number 5 | batch number 1625 | generator loss = 6.979865074157715 | discriminator loss = 0.14975512027740479\n",
      "epoch number 5 | batch number 1825 | generator loss = 1.576040506362915 | discriminator loss = 0.497892290353775\n",
      "epoch number 6 | batch number 150 | generator loss = 2.564958095550537 | discriminator loss = 0.08723322302103043\n",
      "epoch number 6 | batch number 350 | generator loss = 5.80458402633667 | discriminator loss = 0.271330326795578\n",
      "epoch number 6 | batch number 550 | generator loss = 3.8853626251220703 | discriminator loss = 0.007763504050672054\n",
      "epoch number 6 | batch number 750 | generator loss = 5.083126068115234 | discriminator loss = 0.4810650944709778\n",
      "epoch number 6 | batch number 950 | generator loss = 1.4592530727386475 | discriminator loss = 0.13105861842632294\n",
      "epoch number 6 | batch number 1150 | generator loss = 9.902795791625977 | discriminator loss = 0.5222859382629395\n",
      "epoch number 6 | batch number 1350 | generator loss = 2.075732707977295 | discriminator loss = 0.08797181397676468\n",
      "epoch number 6 | batch number 1550 | generator loss = 5.626227855682373 | discriminator loss = 0.5173814296722412\n",
      "epoch number 6 | batch number 1750 | generator loss = 3.339526891708374 | discriminator loss = 0.01968494802713394\n",
      "epoch number 7 | batch number 75 | generator loss = 5.730411052703857 | discriminator loss = 0.08426925539970398\n",
      "epoch number 7 | batch number 275 | generator loss = 1.936887264251709 | discriminator loss = 0.07130797207355499\n",
      "epoch number 7 | batch number 475 | generator loss = 3.2400190830230713 | discriminator loss = 0.10665255039930344\n",
      "epoch number 7 | batch number 675 | generator loss = 5.736668109893799 | discriminator loss = 0.08574121445417404\n",
      "epoch number 7 | batch number 875 | generator loss = 2.715555191040039 | discriminator loss = 0.1132679283618927\n",
      "epoch number 7 | batch number 1075 | generator loss = 2.3564791679382324 | discriminator loss = 0.4902929663658142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number 7 | batch number 1275 | generator loss = 6.810000419616699 | discriminator loss = 0.02540505677461624\n",
      "epoch number 7 | batch number 1475 | generator loss = 3.4194297790527344 | discriminator loss = 0.05772499740123749\n",
      "epoch number 7 | batch number 1675 | generator loss = 5.243895053863525 | discriminator loss = 0.07858455181121826\n",
      "epoch number 8 | batch number 0 | generator loss = 8.872344017028809 | discriminator loss = 0.04475048556923866\n",
      "epoch number 8 | batch number 200 | generator loss = 4.52398681640625 | discriminator loss = 0.814644992351532\n",
      "epoch number 8 | batch number 400 | generator loss = 8.041714668273926 | discriminator loss = 0.06393842399120331\n",
      "epoch number 8 | batch number 600 | generator loss = 5.666053771972656 | discriminator loss = 0.17756833136081696\n",
      "epoch number 8 | batch number 800 | generator loss = 2.8670425415039062 | discriminator loss = 0.033591609448194504\n",
      "epoch number 8 | batch number 1000 | generator loss = 4.065805435180664 | discriminator loss = 0.004144251346588135\n",
      "epoch number 8 | batch number 1200 | generator loss = 5.342024326324463 | discriminator loss = 0.06934899091720581\n",
      "epoch number 8 | batch number 1400 | generator loss = 5.281961917877197 | discriminator loss = 0.10959438979625702\n",
      "epoch number 8 | batch number 1600 | generator loss = 4.389182090759277 | discriminator loss = 0.17440567910671234\n",
      "epoch number 8 | batch number 1800 | generator loss = 1.592084527015686 | discriminator loss = 0.005320039577782154\n",
      "epoch number 9 | batch number 125 | generator loss = 8.134845733642578 | discriminator loss = 0.09175357222557068\n",
      "epoch number 9 | batch number 325 | generator loss = 8.020050048828125 | discriminator loss = 0.09338749945163727\n",
      "epoch number 9 | batch number 525 | generator loss = 5.788740158081055 | discriminator loss = 0.08365482091903687\n",
      "epoch number 9 | batch number 725 | generator loss = 2.1058220863342285 | discriminator loss = 0.8137051463127136\n",
      "epoch number 9 | batch number 925 | generator loss = 5.026509761810303 | discriminator loss = 0.4158681631088257\n",
      "epoch number 9 | batch number 1125 | generator loss = 2.582949638366699 | discriminator loss = 0.08947458863258362\n",
      "epoch number 9 | batch number 1325 | generator loss = 2.019848585128784 | discriminator loss = 0.7362138032913208\n",
      "epoch number 9 | batch number 1525 | generator loss = 6.319630146026611 | discriminator loss = 0.10377539694309235\n",
      "epoch number 9 | batch number 1725 | generator loss = 1.8041318655014038 | discriminator loss = 0.6188918352127075\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"./images_mnist\", exist_ok=True)\n",
    "\n",
    "for ep in range(num_eps):\n",
    "    for idx, (images, _) in enumerate(dloader):\n",
    "\n",
    "        # generate grounnd truths for real and fake images\n",
    "        good_img = Variable(torch.FloatTensor(images.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "        bad_img = Variable(torch.FloatTensor(images.shape[0], 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # get a real image\n",
    "        actual_images = Variable(images.type(torch.FloatTensor))\n",
    "\n",
    "        # train the generator model\n",
    "        opt_gen.zero_grad()\n",
    "\n",
    "        # generate a batch of images based on random noise as input\n",
    "        noise = Variable(torch.FloatTensor(np.random.normal(0, 1, (images.shape[0], lat_dimension))))\n",
    "        gen_images = gen(noise)\n",
    "\n",
    "        # generator model optimization - how well can it fool the discriminator\n",
    "        generator_loss = adv_loss_func(disc(gen_images), good_img)\n",
    "        generator_loss.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # train the discriminator model\n",
    "        opt_disc.zero_grad()\n",
    "\n",
    "        # calculate discriminator loss as average of mistakes(losses) in confusing real images as fake and vice versa\n",
    "        actual_image_loss = adv_loss_func(disc(actual_images), good_img)\n",
    "        fake_image_loss = adv_loss_func(disc(gen_images.detach()), bad_img)\n",
    "        discriminator_loss = (actual_image_loss + fake_image_loss) / 2\n",
    "\n",
    "        # discriminator model optimization\n",
    "        discriminator_loss.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        batches_completed = ep * len(dloader) + idx\n",
    "        if batches_completed % logging_intv == 0:\n",
    "            print(f\"epoch number {ep} | batch number {idx} | generator loss = {generator_loss.item()} | discriminator loss = {discriminator_loss.item()}\")\n",
    "            save_image(gen_images.data[:25], f\"images_mnist/{batches_completed}.png\", nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
