{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torch==1.12\n",
      "  Using cached torch-1.12.0-cp38-none-macosx_10_9_x86_64.whl (137.6 MB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/site-packages (from torch==1.12) (4.4.0)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.10.1\n",
      "    Uninstalling torch-1.10.1:\n",
      "      Successfully uninstalled torch-1.10.1\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.1 requires torch==1.12.1, but you have torch 1.12.0 which is incompatible.\n",
      "torchtext 0.11.1 requires torch==1.10.1, but you have torch 1.12.0 which is incompatible.\n",
      "torchaudio 0.12.1 requires torch==1.12.1, but you have torch 1.12.0 which is incompatible.\n",
      "functorch 0.2.1 requires torch<1.13,>=1.12.1, but you have torch 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.12.0\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting nltk==3.7\n",
      "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.8/site-packages (from nltk==3.7) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /Users/ashish.jha/Library/Python/3.8/lib/python/site-packages (from nltk==3.7) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/site-packages (from nltk==3.7) (2022.9.13)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.8/site-packages (from nltk==3.7) (1.2.0)\n",
      "Installing collected packages: nltk\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed nltk-3.7\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torchtext==0.13.1\n",
      "  Using cached torchtext-0.13.1-cp38-cp38-macosx_10_9_x86_64.whl (1.8 MB)\n",
      "Requirement already satisfied: tqdm in /Users/ashish.jha/Library/Python/3.8/lib/python/site-packages (from torchtext==0.13.1) (4.64.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from torchtext==0.13.1) (2.28.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/site-packages (from torchtext==0.13.1) (1.22.4)\n",
      "Collecting torch==1.12.1\n",
      "  Using cached torch-1.12.1-cp38-none-macosx_10_9_x86_64.whl (137.8 MB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/site-packages (from torch==1.12.1->torchtext==0.13.1) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->torchtext==0.13.1) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->torchtext==0.13.1) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->torchtext==0.13.1) (2.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/site-packages (from requests->torchtext==0.13.1) (2.1.1)\n",
      "Installing collected packages: torch, torchtext\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.0\n",
      "    Uninstalling torch-1.12.0:\n",
      "      Successfully uninstalled torch-1.12.0\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.11.1\n",
      "    Uninstalling torchtext-0.11.1:\n",
      "      Successfully uninstalled torchtext-0.11.1\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchdata 0.4.0 requires torch==1.12.0, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.12.1 torchtext-0.13.1\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torchdata==0.4.1\n",
      "  Using cached torchdata-0.4.1-cp38-cp38-macosx_10_13_x86_64.whl (1.6 MB)\n",
      "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.8/site-packages (from torchdata==0.4.1) (1.26.5)\n",
      "Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.8/site-packages (from torchdata==0.4.1) (1.12.1)\n",
      "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.8/site-packages (from torchdata==0.4.1) (2.6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from torchdata==0.4.1) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/site-packages (from torch==1.12.1->torchdata==0.4.1) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->torchdata==0.4.1) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->torchdata==0.4.1) (2.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/site-packages (from requests->torchdata==0.4.1) (2.1.1)\n",
      "Installing collected packages: torchdata\n",
      "  Attempting uninstall: torchdata\n",
      "    Found existing installation: torchdata 0.4.0\n",
      "    Uninstalling torchdata-0.4.0:\n",
      "      Successfully uninstalled torchdata-0.4.0\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed torchdata-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.12.1\n",
    "!pip install torchtext==0.13.1\n",
    "!pip install torchdata==0.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_token, num_inputs, num_heads, num_hidden, num_layers, dropout=0.3):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.model_name = 'transformer'\n",
    "        self.position_enc = PosEnc(num_inputs, dropout)\n",
    "        layers_enc = TransformerEncoderLayer(num_inputs, num_heads, num_hidden, dropout)\n",
    "        self.enc_transformer = TransformerEncoder(layers_enc, num_layers)\n",
    "        self.enc = nn.Embedding(num_token, num_inputs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.dec = nn.Linear(num_inputs, num_token)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        initial_rng = 0.12\n",
    "        self.enc.weight.data.uniform_(-initial_rng, initial_rng)\n",
    "        self.dec.bias.data.zero_()\n",
    "        self.dec.weight.data.uniform_(-initial_rng, initial_rng)\n",
    "\n",
    "    def forward(self, source, mask_source):\n",
    "        source = self.enc(source) * math.sqrt(self.num_inputs)\n",
    "        source = self.position_enc(source)\n",
    "        op = self.enc_transformer(source, mask_source)\n",
    "        op = self.dec(op)\n",
    "        return op\n",
    "\n",
    "def gen_sqr_nxt_mask(size):\n",
    "    msk = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)\n",
    "    return msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEnc(nn.Module):\n",
    "    def __init__(self, d_m, dropout=0.2, size_limit=5000):\n",
    "        super(PosEnc, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        p_enc = torch.zeros(size_limit, 1, d_m)\n",
    "        pos = torch.arange(size_limit, dtype=torch.float).unsqueeze(1)\n",
    "        divider = torch.exp(torch.arange(0, d_m, 2).float() * (-math.log(10000.0) / d_m))\n",
    "        p_enc[:, 0, 0::2] = torch.sin(pos * divider)\n",
    "        p_enc[:, 0, 1::2] = torch.cos(pos * divider)\n",
    "        self.register_buffer('p_enc', p_enc)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.p_enc[:x.size(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_iter = WikiText2(split='train')\n",
    "tkzer = get_tokenizer('basic_english')\n",
    "vocabulary = build_vocab_from_iterator(map(tkzer, tr_iter), specials=['<unk>'])\n",
    "vocabulary.set_default_index(vocabulary['<unk>'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def process_data(raw_text):\n",
    "    numericalised_text = [torch.tensor(vocabulary(tkzer(text)), dtype=torch.long) for text in raw_text]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, numericalised_text)))\n",
    "\n",
    "tr_iter, val_iter, te_iter = WikiText2()\n",
    "training_text = process_data(tr_iter)\n",
    "validation_text = process_data(val_iter)\n",
    "testing_text = process_data(te_iter)\n",
    "\n",
    "def gen_batches(text_dataset, batch_size):\n",
    "    num_batches = text_dataset.size(0) // batch_size\n",
    "    text_dataset = text_dataset[:num_batches * batch_size]\n",
    "    text_dataset = text_dataset.view(batch_size, num_batches).t().contiguous()\n",
    "    return text_dataset.to(device)\n",
    "\n",
    "training_batch_size = 32\n",
    "evaluation_batch_size = 16\n",
    "\n",
    "training_data = gen_batches(training_text, training_batch_size)\n",
    "validation_data = gen_batches(validation_text, evaluation_batch_size)\n",
    "testing_data = gen_batches(testing_text, evaluation_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 64\n",
    "def return_batch(src, k):\n",
    "    sequence_length = min(max_seq_len, len(src) - 1 - k)\n",
    "    sequence_data = src[k:k+sequence_length]\n",
    "    sequence_label = src[k+1:k+1+sequence_length].reshape(-1)\n",
    "    return sequence_data, sequence_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(vocabulary) # vocabulary size\n",
    "embedding_size = 256 # dimension of embedding layer\n",
    "num_hidden_params = 256 # transformer encoder's hidden (feed forward) layer dimension\n",
    "num_layers = 2 # num of transformer encoder layers within transformer encoder\n",
    "num_heads = 2 # num of heads in (multi head) attention models\n",
    "dropout = 0.25 # value (fraction) of dropout\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "lrate = 4.0 # learning rate\n",
    "transformer_model = Transformer(num_tokens, embedding_size, num_heads, num_hidden_params, num_layers, \n",
    "                                     dropout).to(device)\n",
    "optim_module = torch.optim.SGD(transformer_model.parameters(), lr=lrate)\n",
    "sched_module = torch.optim.lr_scheduler.StepLR(optim_module, 1.0, gamma=0.88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    transformer_model.train()\n",
    "    loss_total = 0.\n",
    "    time_start = time.time()\n",
    "    mask_source = gen_sqr_nxt_mask(max_seq_len).to(device)\n",
    "    num_batches = len(training_data) // max_seq_len\n",
    "    for b, i in enumerate(range(0, training_data.size(0) - 1, max_seq_len)):\n",
    "        train_data_batch, train_label_batch = return_batch(training_data, i)\n",
    "        sequence_length = train_data_batch.size(0)\n",
    "        if sequence_length != max_seq_len:  # only on last batch\n",
    "            mask_source = mask_source[:sequence_length, :sequence_length]\n",
    "        op = transformer_model(train_data_batch, mask_source)\n",
    "        loss_curr = loss_func(op.view(-1, num_tokens), train_label_batch)\n",
    "        optim_module.zero_grad()\n",
    "        loss_curr.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), 0.6)\n",
    "        optim_module.step()\n",
    "\n",
    "        loss_total += loss_curr.item()\n",
    "        interval = 100\n",
    "        if b % interval == 0 and b > 0:\n",
    "            loss_interval = loss_total / interval\n",
    "            time_delta = time.time() - time_start\n",
    "            print(f\"epoch {ep}, {b}/{len(training_data)//max_seq_len} batches, training loss {loss_interval:.2f}, training perplexity {math.exp(loss_interval):.2f}\")\n",
    "            loss_total = 0\n",
    "            time_start = time.time()\n",
    "\n",
    "def eval_model(eval_model_obj, eval_data_source):\n",
    "    eval_model_obj.eval() \n",
    "    loss_total = 0.\n",
    "    mask_source = gen_sqr_nxt_mask(max_seq_len).to(device)\n",
    "    with torch.no_grad():\n",
    "        for j in range(0, eval_data_source.size(0) - 1, max_seq_len):\n",
    "            eval_data, eval_label = return_batch(eval_data_source, j)\n",
    "            sequence_length = eval_data.size(0)\n",
    "            if sequence_length != max_seq_len:\n",
    "                mask_source = mask_source[:sequence_length, :sequence_length]\n",
    "            op = eval_model_obj(eval_data, mask_source)\n",
    "            op_flat = op.view(-1, num_tokens)\n",
    "            loss_total += sequence_length * loss_func(op_flat, eval_label).item()\n",
    "    return loss_total / (len(eval_data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, 100/1000 batches, training loss 8.77, training perplexity 6460.73\n",
      "epoch 1, 200/1000 batches, training loss 7.30, training perplexity 1480.28\n",
      "epoch 1, 300/1000 batches, training loss 6.88, training perplexity 969.18\n",
      "epoch 1, 400/1000 batches, training loss 6.64, training perplexity 764.52\n",
      "epoch 1, 500/1000 batches, training loss 6.54, training perplexity 689.84\n",
      "epoch 1, 600/1000 batches, training loss 6.38, training perplexity 590.40\n",
      "epoch 1, 700/1000 batches, training loss 6.33, training perplexity 559.04\n",
      "epoch 1, 800/1000 batches, training loss 6.20, training perplexity 493.46\n",
      "epoch 1, 900/1000 batches, training loss 6.17, training perplexity 478.56\n",
      "epoch 1, 1000/1000 batches, training loss 6.16, training perplexity 471.33\n",
      "\n",
      "epoch 1, validation loss 5.89, validation perplexity 362.39\n",
      "\n",
      "epoch 2, 100/1000 batches, training loss 6.05, training perplexity 424.82\n",
      "epoch 2, 200/1000 batches, training loss 5.98, training perplexity 396.19\n",
      "epoch 2, 300/1000 batches, training loss 5.90, training perplexity 366.76\n",
      "epoch 2, 400/1000 batches, training loss 5.86, training perplexity 352.10\n",
      "epoch 2, 500/1000 batches, training loss 5.89, training perplexity 360.51\n",
      "epoch 2, 600/1000 batches, training loss 5.84, training perplexity 342.24\n",
      "epoch 2, 700/1000 batches, training loss 5.84, training perplexity 343.03\n",
      "epoch 2, 800/1000 batches, training loss 5.72, training perplexity 306.37\n",
      "epoch 2, 900/1000 batches, training loss 5.75, training perplexity 315.19\n",
      "epoch 2, 1000/1000 batches, training loss 5.78, training perplexity 323.92\n",
      "\n",
      "epoch 2, validation loss 5.63, validation perplexity 279.96\n",
      "\n",
      "epoch 3, 100/1000 batches, training loss 5.73, training perplexity 308.51\n",
      "epoch 3, 200/1000 batches, training loss 5.66, training perplexity 288.48\n",
      "epoch 3, 300/1000 batches, training loss 5.62, training perplexity 274.87\n",
      "epoch 3, 400/1000 batches, training loss 5.59, training perplexity 266.97\n",
      "epoch 3, 500/1000 batches, training loss 5.62, training perplexity 275.49\n",
      "epoch 3, 600/1000 batches, training loss 5.58, training perplexity 265.42\n",
      "epoch 3, 700/1000 batches, training loss 5.58, training perplexity 266.24\n",
      "epoch 3, 800/1000 batches, training loss 5.46, training perplexity 235.79\n",
      "epoch 3, 900/1000 batches, training loss 5.51, training perplexity 246.44\n",
      "epoch 3, 1000/1000 batches, training loss 5.57, training perplexity 261.86\n",
      "\n",
      "epoch 3, validation loss 5.46, validation perplexity 236.05\n",
      "\n",
      "epoch 4, 100/1000 batches, training loss 5.52, training perplexity 249.84\n",
      "epoch 4, 200/1000 batches, training loss 5.47, training perplexity 236.59\n",
      "epoch 4, 300/1000 batches, training loss 5.43, training perplexity 227.94\n",
      "epoch 4, 400/1000 batches, training loss 5.40, training perplexity 221.56\n",
      "epoch 4, 500/1000 batches, training loss 5.43, training perplexity 228.60\n",
      "epoch 4, 600/1000 batches, training loss 5.40, training perplexity 222.47\n",
      "epoch 4, 700/1000 batches, training loss 5.41, training perplexity 222.78\n",
      "epoch 4, 800/1000 batches, training loss 5.27, training perplexity 195.33\n",
      "epoch 4, 900/1000 batches, training loss 5.33, training perplexity 206.06\n",
      "epoch 4, 1000/1000 batches, training loss 5.40, training perplexity 221.73\n",
      "\n",
      "epoch 4, validation loss 5.39, validation perplexity 218.91\n",
      "\n",
      "epoch 5, 100/1000 batches, training loss 5.36, training perplexity 213.62\n",
      "epoch 5, 200/1000 batches, training loss 5.30, training perplexity 201.19\n",
      "epoch 5, 300/1000 batches, training loss 5.28, training perplexity 195.47\n",
      "epoch 5, 400/1000 batches, training loss 5.26, training perplexity 192.86\n",
      "epoch 5, 500/1000 batches, training loss 5.28, training perplexity 197.32\n",
      "epoch 5, 600/1000 batches, training loss 5.26, training perplexity 192.40\n",
      "epoch 5, 700/1000 batches, training loss 5.26, training perplexity 193.29\n",
      "epoch 5, 800/1000 batches, training loss 5.13, training perplexity 169.04\n",
      "epoch 5, 900/1000 batches, training loss 5.19, training perplexity 178.59\n",
      "epoch 5, 1000/1000 batches, training loss 5.27, training perplexity 193.60\n",
      "\n",
      "epoch 5, validation loss 5.32, validation perplexity 204.29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_validation_loss = float(\"inf\")\n",
    "eps = 5\n",
    "best_model_so_far = None\n",
    "\n",
    "for ep in range(1, eps + 1):\n",
    "    ep_time_start = time.time()\n",
    "    train_model()\n",
    "    validation_loss = eval_model(transformer_model, validation_data)\n",
    "    print()\n",
    "    print(f\"epoch {ep:}, validation loss {validation_loss:.2f}, validation perplexity {math.exp(validation_loss):.2f}\")\n",
    "    print()\n",
    "\n",
    "    if validation_loss < min_validation_loss:\n",
    "        min_validation_loss = validation_loss\n",
    "        best_model_so_far = transformer_model\n",
    "\n",
    "    sched_module.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing loss 5.23, testing perplexity 187.45\n"
     ]
    }
   ],
   "source": [
    "testing_loss = eval_model(best_model_so_far, testing_data)\n",
    "print(f\"testing loss {testing_loss:.2f}, testing perplexity {math.exp(testing_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
