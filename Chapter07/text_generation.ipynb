{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torch==1.12\n",
      "  Using cached torch-1.12.0-cp39-none-macosx_10_9_x86_64.whl (133.6 MB)\n",
      "Requirement already satisfied: typing-extensions in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from torch==1.12) (4.4.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: torch 1.12.1\n",
      "    Uninstalling torch-1.12.1:\n",
      "      Successfully uninstalled torch-1.12.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.12.0 which is incompatible.\n",
      "torchdata 0.4.1 requires torch==1.12.1, but you have torch 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.12.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nltk==3.7 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: click in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from nltk==3.7) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from nltk==3.7) (2022.9.13)\n",
      "Requirement already satisfied: tqdm in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from nltk==3.7) (4.64.1)\n",
      "Requirement already satisfied: joblib in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from nltk==3.7) (1.2.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchtext==0.13.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (0.13.1)\n",
      "Collecting torch==1.12.1\n",
      "  Using cached torch-1.12.1-cp39-none-macosx_10_9_x86_64.whl (133.8 MB)\n",
      "Requirement already satisfied: numpy in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from torchtext==0.13.1) (1.22.4)\n",
      "Requirement already satisfied: requests in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from torchtext==0.13.1) (2.28.1)\n",
      "Requirement already satisfied: tqdm in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from torchtext==0.13.1) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from torch==1.12.1->torchtext==0.13.1) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from requests->torchtext==0.13.1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from requests->torchtext==0.13.1) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from requests->torchtext==0.13.1) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from requests->torchtext==0.13.1) (1.26.12)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: torch 1.12.0\n",
      "    Uninstalling torch-1.12.0:\n",
      "      Successfully uninstalled torch-1.12.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.0 requires torch==1.12.0, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-1.12.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchdata==0.4.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (0.4.1)\n",
      "Requirement already satisfied: requests in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from torchdata==0.4.1) (2.28.1)\n",
      "Requirement already satisfied: portalocker>=2.0.0 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from torchdata==0.4.1) (2.6.0)\n",
      "Requirement already satisfied: torch==1.12.1 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from torchdata==0.4.1) (1.12.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from torchdata==0.4.1) (1.26.12)\n",
      "Requirement already satisfied: typing-extensions in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from torch==1.12.1->torchdata==0.4.1) (4.4.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from requests->torchdata==0.4.1) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from requests->torchdata==0.4.1) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages (from requests->torchdata==0.4.1) (2022.9.24)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/Users/ashish.jha/opt/anaconda3/envs/mastering_pytorch_7_chaps/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.12.1\n",
    "!pip install torchtext==0.13.1\n",
    "!pip install torchdata==0.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_token, num_inputs, num_heads, num_hidden, num_layers, dropout=0.3):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.model_name = 'transformer'\n",
    "        self.position_enc = PosEnc(num_inputs, dropout)\n",
    "        layers_enc = TransformerEncoderLayer(num_inputs, num_heads, num_hidden, dropout)\n",
    "        self.enc_transformer = TransformerEncoder(layers_enc, num_layers)\n",
    "        self.enc = nn.Embedding(num_token, num_inputs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.dec = nn.Linear(num_inputs, num_token)\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        initial_rng = 0.12\n",
    "        self.enc.weight.data.uniform_(-initial_rng, initial_rng)\n",
    "        self.dec.bias.data.zero_()\n",
    "        self.dec.weight.data.uniform_(-initial_rng, initial_rng)\n",
    "\n",
    "    def forward(self, source, mask_source):\n",
    "        source = self.enc(source) * math.sqrt(self.num_inputs)\n",
    "        source = self.position_enc(source)\n",
    "        op = self.enc_transformer(source, mask_source)\n",
    "        op = self.dec(op)\n",
    "        return op\n",
    "\n",
    "def gen_sqr_nxt_mask(size):\n",
    "    msk = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)\n",
    "    return msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEnc(nn.Module):\n",
    "    def __init__(self, d_m, dropout=0.2, size_limit=5000):\n",
    "        super(PosEnc, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        p_enc = torch.zeros(size_limit, 1, d_m)\n",
    "        pos = torch.arange(size_limit, dtype=torch.float).unsqueeze(1)\n",
    "        divider = torch.exp(torch.arange(0, d_m, 2).float() * (-math.log(10000.0) / d_m))\n",
    "        p_enc[:, 0, 0::2] = torch.sin(pos * divider)\n",
    "        p_enc[:, 0, 1::2] = torch.cos(pos * divider)\n",
    "        self.register_buffer('p_enc', p_enc)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.p_enc[:x.size(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.48M/4.48M [00:00<00:00, 5.08MB/s]\n"
     ]
    }
   ],
   "source": [
    "tr_iter = WikiText2(split='train')\n",
    "tkzer = get_tokenizer('basic_english')\n",
    "vocabulary = build_vocab_from_iterator(map(tkzer, tr_iter), specials=['<unk>'])\n",
    "vocabulary.set_default_index(vocabulary['<unk>'])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def process_data(raw_text):\n",
    "    numericalised_text = [torch.tensor(vocabulary(tkzer(text)), dtype=torch.long) for text in raw_text]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, numericalised_text)))\n",
    "\n",
    "tr_iter, val_iter, te_iter = WikiText2()\n",
    "training_text = process_data(tr_iter)\n",
    "validation_text = process_data(val_iter)\n",
    "testing_text = process_data(te_iter)\n",
    "\n",
    "def gen_batches(text_dataset, batch_size):\n",
    "    num_batches = text_dataset.size(0) // batch_size\n",
    "    text_dataset = text_dataset[:num_batches * batch_size]\n",
    "    text_dataset = text_dataset.view(batch_size, num_batches).t().contiguous()\n",
    "    return text_dataset.to(device)\n",
    "\n",
    "training_batch_size = 32\n",
    "evaluation_batch_size = 16\n",
    "\n",
    "training_data = gen_batches(training_text, training_batch_size)\n",
    "validation_data = gen_batches(validation_text, evaluation_batch_size)\n",
    "testing_data = gen_batches(testing_text, evaluation_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 64\n",
    "def return_batch(src, k):\n",
    "    sequence_length = min(max_seq_len, len(src) - 1 - k)\n",
    "    sequence_data = src[k:k+sequence_length]\n",
    "    sequence_label = src[k+1:k+1+sequence_length].reshape(-1)\n",
    "    return sequence_data, sequence_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(vocabulary) # vocabulary size\n",
    "embedding_size = 256 # dimension of embedding layer\n",
    "num_hidden_params = 256 # transformer encoder's hidden (feed forward) layer dimension\n",
    "num_layers = 2 # num of transformer encoder layers within transformer encoder\n",
    "num_heads = 2 # num of heads in (multi head) attention models\n",
    "dropout = 0.25 # value (fraction) of dropout\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "lrate = 4.0 # learning rate\n",
    "transformer_model = Transformer(num_tokens, embedding_size, num_heads, num_hidden_params, num_layers, \n",
    "                                     dropout).to(device)\n",
    "optim_module = torch.optim.SGD(transformer_model.parameters(), lr=lrate)\n",
    "sched_module = torch.optim.lr_scheduler.StepLR(optim_module, 1.0, gamma=0.88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    transformer_model.train()\n",
    "    loss_total = 0.\n",
    "    time_start = time.time()\n",
    "    mask_source = gen_sqr_nxt_mask(max_seq_len).to(device)\n",
    "    num_batches = len(training_data) // max_seq_len\n",
    "    for b, i in enumerate(range(0, training_data.size(0) - 1, max_seq_len)):\n",
    "        train_data_batch, train_label_batch = return_batch(training_data, i)\n",
    "        sequence_length = train_data_batch.size(0)\n",
    "        if sequence_length != max_seq_len:  # only on last batch\n",
    "            mask_source = mask_source[:sequence_length, :sequence_length]\n",
    "        op = transformer_model(train_data_batch, mask_source)\n",
    "        loss_curr = loss_func(op.view(-1, num_tokens), train_label_batch)\n",
    "        optim_module.zero_grad()\n",
    "        loss_curr.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), 0.6)\n",
    "        optim_module.step()\n",
    "\n",
    "        loss_total += loss_curr.item()\n",
    "        interval = 100\n",
    "        if b % interval == 0 and b > 0:\n",
    "            loss_interval = loss_total / interval\n",
    "            time_delta = time.time() - time_start\n",
    "            print(f\"epoch {ep}, {b}/{len(training_data)//max_seq_len} batches, training loss {loss_interval:.2f}, training perplexity {math.exp(loss_interval):.2f}\")\n",
    "            loss_total = 0\n",
    "            time_start = time.time()\n",
    "\n",
    "def eval_model(eval_model_obj, eval_data_source):\n",
    "    eval_model_obj.eval() \n",
    "    loss_total = 0.\n",
    "    mask_source = gen_sqr_nxt_mask(max_seq_len).to(device)\n",
    "    with torch.no_grad():\n",
    "        for j in range(0, eval_data_source.size(0) - 1, max_seq_len):\n",
    "            eval_data, eval_label = return_batch(eval_data_source, j)\n",
    "            sequence_length = eval_data.size(0)\n",
    "            if sequence_length != max_seq_len:\n",
    "                mask_source = mask_source[:sequence_length, :sequence_length]\n",
    "            op = eval_model_obj(eval_data, mask_source)\n",
    "            op_flat = op.view(-1, num_tokens)\n",
    "            loss_total += sequence_length * loss_func(op_flat, eval_label).item()\n",
    "    return loss_total / (len(eval_data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, 100/1000 batches, training loss 8.81, training perplexity 6724.85\n",
      "epoch 1, 200/1000 batches, training loss 7.35, training perplexity 1555.26\n",
      "epoch 1, 300/1000 batches, training loss 6.90, training perplexity 991.85\n",
      "epoch 1, 400/1000 batches, training loss 6.67, training perplexity 792.05\n",
      "epoch 1, 500/1000 batches, training loss 6.54, training perplexity 694.65\n",
      "epoch 1, 600/1000 batches, training loss 6.39, training perplexity 597.00\n",
      "epoch 1, 700/1000 batches, training loss 6.34, training perplexity 569.06\n",
      "epoch 1, 800/1000 batches, training loss 6.21, training perplexity 498.15\n",
      "epoch 1, 900/1000 batches, training loss 6.18, training perplexity 485.01\n",
      "epoch 1, 1000/1000 batches, training loss 6.16, training perplexity 472.72\n",
      "\n",
      "epoch 1, validation loss 5.87, validation perplexity 353.51\n",
      "\n",
      "epoch 2, 100/1000 batches, training loss 6.07, training perplexity 430.66\n",
      "epoch 2, 200/1000 batches, training loss 5.99, training perplexity 399.61\n",
      "epoch 2, 300/1000 batches, training loss 5.91, training perplexity 369.30\n",
      "epoch 2, 400/1000 batches, training loss 5.88, training perplexity 357.08\n",
      "epoch 2, 500/1000 batches, training loss 5.90, training perplexity 366.79\n",
      "epoch 2, 600/1000 batches, training loss 5.84, training perplexity 342.41\n",
      "epoch 2, 700/1000 batches, training loss 5.85, training perplexity 346.25\n",
      "epoch 2, 800/1000 batches, training loss 5.73, training perplexity 308.89\n",
      "epoch 2, 900/1000 batches, training loss 5.76, training perplexity 315.81\n",
      "epoch 2, 1000/1000 batches, training loss 5.80, training perplexity 329.08\n",
      "\n",
      "epoch 2, validation loss 5.72, validation perplexity 306.07\n",
      "\n",
      "epoch 3, 100/1000 batches, training loss 5.75, training perplexity 314.65\n",
      "epoch 3, 200/1000 batches, training loss 5.69, training perplexity 295.20\n",
      "epoch 3, 300/1000 batches, training loss 5.63, training perplexity 277.60\n",
      "epoch 3, 400/1000 batches, training loss 5.60, training perplexity 271.04\n",
      "epoch 3, 500/1000 batches, training loss 5.63, training perplexity 278.04\n",
      "epoch 3, 600/1000 batches, training loss 5.60, training perplexity 269.19\n",
      "epoch 3, 700/1000 batches, training loss 5.60, training perplexity 269.97\n",
      "epoch 3, 800/1000 batches, training loss 5.47, training perplexity 237.38\n",
      "epoch 3, 900/1000 batches, training loss 5.52, training perplexity 248.73\n",
      "epoch 3, 1000/1000 batches, training loss 5.58, training perplexity 263.76\n",
      "\n",
      "epoch 3, validation loss 5.49, validation perplexity 241.20\n",
      "\n",
      "epoch 4, 100/1000 batches, training loss 5.54, training perplexity 255.03\n",
      "epoch 4, 200/1000 batches, training loss 5.48, training perplexity 239.79\n",
      "epoch 4, 300/1000 batches, training loss 5.44, training perplexity 229.90\n",
      "epoch 4, 400/1000 batches, training loss 5.42, training perplexity 225.38\n",
      "epoch 4, 500/1000 batches, training loss 5.44, training perplexity 230.14\n",
      "epoch 4, 600/1000 batches, training loss 5.42, training perplexity 224.80\n",
      "epoch 4, 700/1000 batches, training loss 5.42, training perplexity 225.99\n",
      "epoch 4, 800/1000 batches, training loss 5.28, training perplexity 196.63\n",
      "epoch 4, 900/1000 batches, training loss 5.34, training perplexity 209.03\n",
      "epoch 4, 1000/1000 batches, training loss 5.41, training perplexity 223.52\n",
      "\n",
      "epoch 4, validation loss 5.45, validation perplexity 232.87\n",
      "\n",
      "epoch 5, 100/1000 batches, training loss 5.38, training perplexity 217.33\n",
      "epoch 5, 200/1000 batches, training loss 5.32, training perplexity 204.41\n",
      "epoch 5, 300/1000 batches, training loss 5.29, training perplexity 199.01\n",
      "epoch 5, 400/1000 batches, training loss 5.27, training perplexity 195.19\n",
      "epoch 5, 500/1000 batches, training loss 5.30, training perplexity 200.11\n",
      "epoch 5, 600/1000 batches, training loss 5.27, training perplexity 195.02\n",
      "epoch 5, 700/1000 batches, training loss 5.28, training perplexity 196.36\n",
      "epoch 5, 800/1000 batches, training loss 5.14, training perplexity 170.20\n",
      "epoch 5, 900/1000 batches, training loss 5.20, training perplexity 181.51\n",
      "epoch 5, 1000/1000 batches, training loss 5.27, training perplexity 195.02\n",
      "\n",
      "epoch 5, validation loss 5.32, validation perplexity 204.73\n",
      "\n",
      "epoch 6, 100/1000 batches, training loss 5.25, training perplexity 190.96\n",
      "epoch 6, 200/1000 batches, training loss 5.19, training perplexity 179.41\n",
      "epoch 6, 300/1000 batches, training loss 5.17, training perplexity 175.67\n",
      "epoch 6, 400/1000 batches, training loss 5.16, training perplexity 174.29\n",
      "epoch 6, 500/1000 batches, training loss 5.18, training perplexity 176.86\n",
      "epoch 6, 600/1000 batches, training loss 5.15, training perplexity 173.19\n",
      "epoch 6, 700/1000 batches, training loss 5.16, training perplexity 173.53\n",
      "epoch 6, 800/1000 batches, training loss 5.02, training perplexity 150.71\n",
      "epoch 6, 900/1000 batches, training loss 5.08, training perplexity 160.35\n",
      "epoch 6, 1000/1000 batches, training loss 5.16, training perplexity 174.83\n",
      "\n",
      "epoch 6, validation loss 5.28, validation perplexity 195.39\n",
      "\n",
      "epoch 7, 100/1000 batches, training loss 5.14, training perplexity 170.78\n",
      "epoch 7, 200/1000 batches, training loss 5.08, training perplexity 160.07\n",
      "epoch 7, 300/1000 batches, training loss 5.06, training perplexity 157.82\n",
      "epoch 7, 400/1000 batches, training loss 5.05, training perplexity 156.64\n",
      "epoch 7, 500/1000 batches, training loss 5.07, training perplexity 158.52\n",
      "epoch 7, 600/1000 batches, training loss 5.05, training perplexity 156.05\n",
      "epoch 7, 700/1000 batches, training loss 5.06, training perplexity 157.45\n",
      "epoch 7, 800/1000 batches, training loss 4.91, training perplexity 135.65\n",
      "epoch 7, 900/1000 batches, training loss 4.98, training perplexity 145.26\n",
      "epoch 7, 1000/1000 batches, training loss 5.05, training perplexity 156.51\n",
      "\n",
      "epoch 7, validation loss 5.22, validation perplexity 185.69\n",
      "\n",
      "epoch 8, 100/1000 batches, training loss 5.05, training perplexity 155.52\n",
      "epoch 8, 200/1000 batches, training loss 4.98, training perplexity 144.76\n",
      "epoch 8, 300/1000 batches, training loss 4.97, training perplexity 144.02\n",
      "epoch 8, 400/1000 batches, training loss 4.96, training perplexity 143.31\n",
      "epoch 8, 500/1000 batches, training loss 4.98, training perplexity 145.44\n",
      "epoch 8, 600/1000 batches, training loss 4.96, training perplexity 142.89\n",
      "epoch 8, 700/1000 batches, training loss 4.96, training perplexity 143.11\n",
      "epoch 8, 800/1000 batches, training loss 4.82, training perplexity 124.53\n",
      "epoch 8, 900/1000 batches, training loss 4.89, training perplexity 132.78\n",
      "epoch 8, 1000/1000 batches, training loss 4.96, training perplexity 142.85\n",
      "\n",
      "epoch 8, validation loss 5.21, validation perplexity 183.43\n",
      "\n",
      "epoch 9, 100/1000 batches, training loss 4.96, training perplexity 143.17\n",
      "epoch 9, 200/1000 batches, training loss 4.90, training perplexity 133.79\n",
      "epoch 9, 300/1000 batches, training loss 4.89, training perplexity 132.80\n",
      "epoch 9, 400/1000 batches, training loss 4.89, training perplexity 132.36\n",
      "epoch 9, 500/1000 batches, training loss 4.90, training perplexity 133.88\n",
      "epoch 9, 600/1000 batches, training loss 4.88, training perplexity 131.89\n",
      "epoch 9, 700/1000 batches, training loss 4.89, training perplexity 132.46\n",
      "epoch 9, 800/1000 batches, training loss 4.75, training perplexity 115.19\n",
      "epoch 9, 900/1000 batches, training loss 4.82, training perplexity 123.43\n",
      "epoch 9, 1000/1000 batches, training loss 4.89, training perplexity 132.41\n",
      "\n",
      "epoch 9, validation loss 5.17, validation perplexity 176.50\n",
      "\n",
      "epoch 10, 100/1000 batches, training loss 4.89, training perplexity 133.34\n",
      "epoch 10, 200/1000 batches, training loss 4.82, training perplexity 124.32\n",
      "epoch 10, 300/1000 batches, training loss 4.82, training perplexity 123.80\n",
      "epoch 10, 400/1000 batches, training loss 4.82, training perplexity 123.85\n",
      "epoch 10, 500/1000 batches, training loss 4.83, training perplexity 125.27\n",
      "epoch 10, 600/1000 batches, training loss 4.82, training perplexity 123.39\n",
      "epoch 10, 700/1000 batches, training loss 4.82, training perplexity 124.25\n",
      "epoch 10, 800/1000 batches, training loss 4.68, training perplexity 108.09\n",
      "epoch 10, 900/1000 batches, training loss 4.75, training perplexity 115.30\n",
      "epoch 10, 1000/1000 batches, training loss 4.82, training perplexity 124.23\n",
      "\n",
      "epoch 10, validation loss 5.15, validation perplexity 172.70\n",
      "\n",
      "epoch 11, 100/1000 batches, training loss 4.83, training perplexity 125.28\n",
      "epoch 11, 200/1000 batches, training loss 4.76, training perplexity 117.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, 300/1000 batches, training loss 4.77, training perplexity 117.61\n",
      "epoch 11, 400/1000 batches, training loss 4.76, training perplexity 117.09\n",
      "epoch 11, 500/1000 batches, training loss 4.77, training perplexity 117.95\n",
      "epoch 11, 600/1000 batches, training loss 4.76, training perplexity 116.57\n",
      "epoch 11, 700/1000 batches, training loss 4.76, training perplexity 117.14\n",
      "epoch 11, 800/1000 batches, training loss 4.63, training perplexity 102.52\n",
      "epoch 11, 900/1000 batches, training loss 4.69, training perplexity 109.21\n",
      "epoch 11, 1000/1000 batches, training loss 4.76, training perplexity 117.24\n",
      "\n",
      "epoch 11, validation loss 5.14, validation perplexity 170.42\n",
      "\n",
      "epoch 12, 100/1000 batches, training loss 4.78, training perplexity 119.25\n",
      "epoch 12, 200/1000 batches, training loss 4.71, training perplexity 111.20\n",
      "epoch 12, 300/1000 batches, training loss 4.72, training perplexity 111.81\n",
      "epoch 12, 400/1000 batches, training loss 4.71, training perplexity 111.42\n",
      "epoch 12, 500/1000 batches, training loss 4.72, training perplexity 112.22\n",
      "epoch 12, 600/1000 batches, training loss 4.71, training perplexity 111.05\n",
      "epoch 12, 700/1000 batches, training loss 4.72, training perplexity 111.82\n",
      "epoch 12, 800/1000 batches, training loss 4.58, training perplexity 97.81\n",
      "epoch 12, 900/1000 batches, training loss 4.65, training perplexity 104.37\n",
      "epoch 12, 1000/1000 batches, training loss 4.72, training perplexity 111.69\n",
      "\n",
      "epoch 12, validation loss 5.13, validation perplexity 168.90\n",
      "\n",
      "epoch 13, 100/1000 batches, training loss 4.74, training perplexity 114.04\n",
      "epoch 13, 200/1000 batches, training loss 4.67, training perplexity 106.32\n",
      "epoch 13, 300/1000 batches, training loss 4.67, training perplexity 107.06\n",
      "epoch 13, 400/1000 batches, training loss 4.67, training perplexity 106.68\n",
      "epoch 13, 500/1000 batches, training loss 4.68, training perplexity 107.57\n",
      "epoch 13, 600/1000 batches, training loss 4.67, training perplexity 106.64\n",
      "epoch 13, 700/1000 batches, training loss 4.68, training perplexity 107.32\n",
      "epoch 13, 800/1000 batches, training loss 4.54, training perplexity 94.14\n",
      "epoch 13, 900/1000 batches, training loss 4.60, training perplexity 99.89\n",
      "epoch 13, 1000/1000 batches, training loss 4.67, training perplexity 106.91\n",
      "\n",
      "epoch 13, validation loss 5.13, validation perplexity 168.43\n",
      "\n",
      "epoch 14, 100/1000 batches, training loss 4.70, training perplexity 109.88\n",
      "epoch 14, 200/1000 batches, training loss 4.63, training perplexity 102.38\n",
      "epoch 14, 300/1000 batches, training loss 4.63, training perplexity 102.98\n",
      "epoch 14, 400/1000 batches, training loss 4.63, training perplexity 102.62\n",
      "epoch 14, 500/1000 batches, training loss 4.64, training perplexity 103.69\n",
      "epoch 14, 600/1000 batches, training loss 4.63, training perplexity 102.77\n",
      "epoch 14, 700/1000 batches, training loss 4.64, training perplexity 103.34\n",
      "epoch 14, 800/1000 batches, training loss 4.51, training perplexity 90.65\n",
      "epoch 14, 900/1000 batches, training loss 4.57, training perplexity 96.32\n",
      "epoch 14, 1000/1000 batches, training loss 4.63, training perplexity 102.66\n",
      "\n",
      "epoch 14, validation loss 5.12, validation perplexity 167.53\n",
      "\n",
      "epoch 15, 100/1000 batches, training loss 4.67, training perplexity 106.44\n",
      "epoch 15, 200/1000 batches, training loss 4.60, training perplexity 99.25\n",
      "epoch 15, 300/1000 batches, training loss 4.61, training perplexity 100.50\n",
      "epoch 15, 400/1000 batches, training loss 4.60, training perplexity 99.81\n",
      "epoch 15, 500/1000 batches, training loss 4.61, training perplexity 100.66\n",
      "epoch 15, 600/1000 batches, training loss 4.60, training perplexity 99.31\n",
      "epoch 15, 700/1000 batches, training loss 4.61, training perplexity 100.22\n",
      "epoch 15, 800/1000 batches, training loss 4.48, training perplexity 88.12\n",
      "epoch 15, 900/1000 batches, training loss 4.54, training perplexity 93.25\n",
      "epoch 15, 1000/1000 batches, training loss 4.60, training perplexity 99.95\n",
      "\n",
      "epoch 15, validation loss 5.11, validation perplexity 166.16\n",
      "\n",
      "epoch 16, 100/1000 batches, training loss 4.64, training perplexity 103.39\n",
      "epoch 16, 200/1000 batches, training loss 4.57, training perplexity 96.48\n",
      "epoch 16, 300/1000 batches, training loss 4.58, training perplexity 97.51\n",
      "epoch 16, 400/1000 batches, training loss 4.57, training perplexity 96.98\n",
      "epoch 16, 500/1000 batches, training loss 4.59, training perplexity 98.08\n",
      "epoch 16, 600/1000 batches, training loss 4.58, training perplexity 97.28\n",
      "epoch 16, 700/1000 batches, training loss 4.58, training perplexity 97.41\n",
      "epoch 16, 800/1000 batches, training loss 4.45, training perplexity 85.81\n",
      "epoch 16, 900/1000 batches, training loss 4.51, training perplexity 90.97\n",
      "epoch 16, 1000/1000 batches, training loss 4.57, training perplexity 97.02\n",
      "\n",
      "epoch 16, validation loss 5.10, validation perplexity 164.65\n",
      "\n",
      "epoch 17, 100/1000 batches, training loss 4.61, training perplexity 100.86\n",
      "epoch 17, 200/1000 batches, training loss 4.54, training perplexity 94.16\n",
      "epoch 17, 300/1000 batches, training loss 4.56, training perplexity 95.25\n",
      "epoch 17, 400/1000 batches, training loss 4.55, training perplexity 94.95\n",
      "epoch 17, 500/1000 batches, training loss 4.56, training perplexity 95.46\n",
      "epoch 17, 600/1000 batches, training loss 4.55, training perplexity 94.90\n",
      "epoch 17, 700/1000 batches, training loss 4.56, training perplexity 95.49\n",
      "epoch 17, 800/1000 batches, training loss 4.43, training perplexity 83.96\n",
      "epoch 17, 900/1000 batches, training loss 4.48, training perplexity 88.59\n",
      "epoch 17, 1000/1000 batches, training loss 4.55, training perplexity 94.55\n",
      "\n",
      "epoch 17, validation loss 5.10, validation perplexity 164.16\n",
      "\n",
      "epoch 18, 100/1000 batches, training loss 4.59, training perplexity 98.75\n",
      "epoch 18, 200/1000 batches, training loss 4.52, training perplexity 92.19\n",
      "epoch 18, 300/1000 batches, training loss 4.54, training perplexity 93.33\n",
      "epoch 18, 400/1000 batches, training loss 4.53, training perplexity 92.82\n",
      "epoch 18, 500/1000 batches, training loss 4.54, training perplexity 93.71\n",
      "epoch 18, 600/1000 batches, training loss 4.53, training perplexity 92.82\n",
      "epoch 18, 700/1000 batches, training loss 4.54, training perplexity 93.34\n",
      "epoch 18, 800/1000 batches, training loss 4.41, training perplexity 82.16\n",
      "epoch 18, 900/1000 batches, training loss 4.47, training perplexity 86.94\n",
      "epoch 18, 1000/1000 batches, training loss 4.53, training perplexity 92.87\n",
      "\n",
      "epoch 18, validation loss 5.10, validation perplexity 163.30\n",
      "\n",
      "epoch 19, 100/1000 batches, training loss 4.58, training perplexity 97.20\n",
      "epoch 19, 200/1000 batches, training loss 4.50, training perplexity 90.26\n",
      "epoch 19, 300/1000 batches, training loss 4.52, training perplexity 91.80\n",
      "epoch 19, 400/1000 batches, training loss 4.52, training perplexity 91.39\n",
      "epoch 19, 500/1000 batches, training loss 4.52, training perplexity 91.97\n",
      "epoch 19, 600/1000 batches, training loss 4.52, training perplexity 91.47\n",
      "epoch 19, 700/1000 batches, training loss 4.52, training perplexity 91.88\n",
      "epoch 19, 800/1000 batches, training loss 4.39, training perplexity 80.87\n",
      "epoch 19, 900/1000 batches, training loss 4.45, training perplexity 85.51\n",
      "epoch 19, 1000/1000 batches, training loss 4.51, training perplexity 91.33\n",
      "\n",
      "epoch 19, validation loss 5.09, validation perplexity 162.86\n",
      "\n",
      "epoch 20, 100/1000 batches, training loss 4.56, training perplexity 95.29\n",
      "epoch 20, 200/1000 batches, training loss 4.49, training perplexity 89.23\n",
      "epoch 20, 300/1000 batches, training loss 4.51, training perplexity 90.53\n",
      "epoch 20, 400/1000 batches, training loss 4.50, training perplexity 90.22\n",
      "epoch 20, 500/1000 batches, training loss 4.51, training perplexity 90.55\n",
      "epoch 20, 600/1000 batches, training loss 4.50, training perplexity 90.02\n",
      "epoch 20, 700/1000 batches, training loss 4.50, training perplexity 90.39\n",
      "epoch 20, 800/1000 batches, training loss 4.38, training perplexity 79.54\n",
      "epoch 20, 900/1000 batches, training loss 4.44, training perplexity 84.51\n",
      "epoch 20, 1000/1000 batches, training loss 4.50, training perplexity 89.80\n",
      "\n",
      "epoch 20, validation loss 5.08, validation perplexity 161.36\n",
      "\n",
      "epoch 21, 100/1000 batches, training loss 4.55, training perplexity 94.34\n",
      "epoch 21, 200/1000 batches, training loss 4.48, training perplexity 87.91\n",
      "epoch 21, 300/1000 batches, training loss 4.49, training perplexity 89.28\n",
      "epoch 21, 400/1000 batches, training loss 4.49, training perplexity 88.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21, 500/1000 batches, training loss 4.49, training perplexity 89.24\n",
      "epoch 21, 600/1000 batches, training loss 4.49, training perplexity 88.83\n",
      "epoch 21, 700/1000 batches, training loss 4.49, training perplexity 89.17\n",
      "epoch 21, 800/1000 batches, training loss 4.37, training perplexity 78.77\n",
      "epoch 21, 900/1000 batches, training loss 4.43, training perplexity 83.60\n",
      "epoch 21, 1000/1000 batches, training loss 4.49, training perplexity 88.95\n",
      "\n",
      "epoch 21, validation loss 5.08, validation perplexity 160.65\n",
      "\n",
      "epoch 22, 100/1000 batches, training loss 4.53, training perplexity 92.97\n",
      "epoch 22, 200/1000 batches, training loss 4.46, training perplexity 86.83\n",
      "epoch 22, 300/1000 batches, training loss 4.48, training perplexity 88.53\n",
      "epoch 22, 400/1000 batches, training loss 4.48, training perplexity 88.05\n",
      "epoch 22, 500/1000 batches, training loss 4.48, training perplexity 88.57\n",
      "epoch 22, 600/1000 batches, training loss 4.47, training perplexity 87.68\n",
      "epoch 22, 700/1000 batches, training loss 4.48, training perplexity 88.22\n",
      "epoch 22, 800/1000 batches, training loss 4.36, training perplexity 78.02\n",
      "epoch 22, 900/1000 batches, training loss 4.41, training perplexity 82.54\n",
      "epoch 22, 1000/1000 batches, training loss 4.48, training perplexity 87.87\n",
      "\n",
      "epoch 22, validation loss 5.08, validation perplexity 160.64\n",
      "\n",
      "epoch 23, 100/1000 batches, training loss 4.53, training perplexity 92.44\n",
      "epoch 23, 200/1000 batches, training loss 4.46, training perplexity 86.46\n",
      "epoch 23, 300/1000 batches, training loss 4.47, training perplexity 87.66\n",
      "epoch 23, 400/1000 batches, training loss 4.47, training perplexity 87.08\n",
      "epoch 23, 500/1000 batches, training loss 4.47, training perplexity 87.43\n",
      "epoch 23, 600/1000 batches, training loss 4.47, training perplexity 87.13\n",
      "epoch 23, 700/1000 batches, training loss 4.47, training perplexity 87.73\n",
      "epoch 23, 800/1000 batches, training loss 4.35, training perplexity 77.17\n",
      "epoch 23, 900/1000 batches, training loss 4.40, training perplexity 81.72\n",
      "epoch 23, 1000/1000 batches, training loss 4.46, training perplexity 86.64\n",
      "\n",
      "epoch 23, validation loss 5.08, validation perplexity 160.08\n",
      "\n",
      "epoch 24, 100/1000 batches, training loss 4.52, training perplexity 91.65\n",
      "epoch 24, 200/1000 batches, training loss 4.45, training perplexity 85.39\n",
      "epoch 24, 300/1000 batches, training loss 4.46, training perplexity 86.55\n",
      "epoch 24, 400/1000 batches, training loss 4.46, training perplexity 86.76\n",
      "epoch 24, 500/1000 batches, training loss 4.47, training perplexity 87.19\n",
      "epoch 24, 600/1000 batches, training loss 4.46, training perplexity 86.27\n",
      "epoch 24, 700/1000 batches, training loss 4.47, training perplexity 86.94\n",
      "epoch 24, 800/1000 batches, training loss 4.34, training perplexity 76.65\n",
      "epoch 24, 900/1000 batches, training loss 4.39, training perplexity 80.96\n",
      "epoch 24, 1000/1000 batches, training loss 4.45, training perplexity 86.01\n",
      "\n",
      "epoch 24, validation loss 5.07, validation perplexity 159.87\n",
      "\n",
      "epoch 25, 100/1000 batches, training loss 4.51, training perplexity 91.09\n",
      "epoch 25, 200/1000 batches, training loss 4.44, training perplexity 85.05\n",
      "epoch 25, 300/1000 batches, training loss 4.46, training perplexity 86.22\n",
      "epoch 25, 400/1000 batches, training loss 4.45, training perplexity 85.91\n",
      "epoch 25, 500/1000 batches, training loss 4.46, training perplexity 86.60\n",
      "epoch 25, 600/1000 batches, training loss 4.45, training perplexity 86.05\n",
      "epoch 25, 700/1000 batches, training loss 4.46, training perplexity 86.37\n",
      "epoch 25, 800/1000 batches, training loss 4.33, training perplexity 76.17\n",
      "epoch 25, 900/1000 batches, training loss 4.39, training perplexity 80.44\n",
      "epoch 25, 1000/1000 batches, training loss 4.45, training perplexity 85.74\n",
      "\n",
      "epoch 25, validation loss 5.07, validation perplexity 159.50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_validation_loss = float(\"inf\")\n",
    "eps = 25\n",
    "best_model_so_far = None\n",
    "\n",
    "for ep in range(1, eps + 1):\n",
    "    ep_time_start = time.time()\n",
    "    train_model()\n",
    "    validation_loss = eval_model(transformer_model, validation_data)\n",
    "    print()\n",
    "    print(f\"epoch {ep:}, validation loss {validation_loss:.2f}, validation perplexity {math.exp(validation_loss):.2f}\")\n",
    "    print()\n",
    "\n",
    "    if validation_loss < min_validation_loss:\n",
    "        min_validation_loss = validation_loss\n",
    "        best_model_so_far = transformer_model\n",
    "\n",
    "    sched_module.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing loss 5.00, testing perplexity 148.86\n"
     ]
    }
   ],
   "source": [
    "testing_loss = eval_model(best_model_so_far, testing_data)\n",
    "print(f\"testing loss {testing_loss:.2f}, testing perplexity {math.exp(testing_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_pth = './transformer.pth'\n",
    "torch.save(best_model_so_far.state_dict(), mdl_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the best trained model\n",
    "transformer_cached = Transformer(num_tokens, embedding_size, num_heads, num_hidden_params, num_layers, \n",
    "                                     dropout).to(device)\n",
    "transformer_cached.load_state_dict(torch.load(mdl_pth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They are often used for the\n"
     ]
    }
   ],
   "source": [
    "ln = 5\n",
    "sntc = 'They are _'\n",
    "sntc_split = sntc.split()\n",
    "torch.manual_seed(34)\n",
    "mask_source = gen_sqr_nxt_mask(max_seq_len).to(device)\n",
    "with torch.no_grad():\n",
    "    for i in range(ln):\n",
    "        sntc = ' '.join(sntc_split)\n",
    "        txt_ds = Tensor(vocabulary(sntc_split)).unsqueeze(0).to(torch.long)\n",
    "        num_b = txt_ds.size(0)\n",
    "        txt_ds = txt_ds.narrow(0, 0, num_b)\n",
    "        txt_ds = txt_ds.view(1, -1).t().contiguous().to(device)\n",
    "        ev_X, _ = return_batch(txt_ds, i+1)\n",
    "        sequence_length = ev_X.size(0)\n",
    "        if sequence_length != max_seq_len:\n",
    "            mask_source = mask_source[:sequence_length, :sequence_length]\n",
    "        op = transformer_cached(ev_X, mask_source)\n",
    "        op_flat = op.view(-1, num_tokens)\n",
    "        res = vocabulary.get_itos()[op_flat.argmax(1)[0]]\n",
    "        sntc_split.insert(-1, res)\n",
    "print(sntc[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
